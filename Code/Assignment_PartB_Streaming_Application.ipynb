{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "faa20bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geohash2 in /opt/conda/lib/python3.8/site-packages (1.1)\r\n",
      "Requirement already satisfied: docutils>=0.3 in /opt/conda/lib/python3.8/site-packages (from geohash2) (0.20.1)\r\n"
     ]
    }
   ],
   "source": [
    "# Install geohash2 for geohashing algorithm\n",
    "!pip install geohash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9dbfa09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, struct, when, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType, TimestampType\n",
    "from pyspark.sql.functions import udf, to_timestamp\n",
    "import geohash2\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n",
    "\n",
    "# Set ip address to host ip address\n",
    "ip_address = \"192.168.224.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fc868aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the data from collection that will be used\n",
    "# client = MongoClient(ip_address, 27017)\n",
    "# db = client[\"A3_db\"]\n",
    "# collection = db[\"ClimateHotspot\"]\n",
    "# collection.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fa866ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined function for calculating geohash\n",
    "def to_geohash(lat, lon, precision=3):\n",
    "    return geohash2.encode(lat, lon, precision)\n",
    "# geohash_udf = udf(lambda lat, lon: geohash2.encode(lat, lon, precision=3), StringType())\n",
    "geohash_udf = udf(to_geohash, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0dbe8c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to accept data into dataframe (for debugging purposes)\n",
    "def receive_data(producer):\n",
    "    try:\n",
    "        query = producer \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"console\") \\\n",
    "        .start() \\\n",
    "        \n",
    "        query.awaitTermination()\n",
    "    except:\n",
    "        print(\"ERROR\")\n",
    "    finally:\n",
    "        query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f4fb4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('FireHotspotConsumer') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a6b308a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the climate data\n",
    "climate_schema = StructType([\n",
    "    StructField(\"date\", StringType(), nullable=True),\n",
    "    StructField(\"latitude\", DoubleType(), nullable=True),\n",
    "    StructField(\"longitude\", DoubleType(), nullable=True),\n",
    "    StructField(\"air_temperature\", DoubleType(), nullable=True),\n",
    "    StructField(\"relative_humidity\", DoubleType(), nullable=True),\n",
    "    StructField(\"windspeed_knots\", DoubleType(), nullable=True),\n",
    "    StructField(\"max_wind_speed\", DoubleType(), nullable=True),\n",
    "    StructField(\"precipitation_flag\", StringType(), nullable=True),\n",
    "    StructField(\"precipitation\", DoubleType(), nullable=True),\n",
    "    StructField(\"GHI\", DoubleType(), nullable=True),\n",
    "    StructField(\"producer\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Define the schema for the hotspot data\n",
    "hotspot_schema = StructType([\n",
    "    StructField(\"date\", StringType(), nullable=True),\n",
    "    StructField(\"datetime\", StringType(), nullable=True),\n",
    "    StructField(\"latitude\", DoubleType(), nullable=True),\n",
    "    StructField(\"longitude\", DoubleType(), nullable=True),\n",
    "    StructField(\"confidence\", DoubleType(), nullable=True),\n",
    "    StructField(\"surface_temperature\", DoubleType(), nullable=True),\n",
    "    StructField(\"producer\", StringType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "96853d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the climate data from Kafka\n",
    "climate_data = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', f'{ip_address}:9092') \\\n",
    "    .option('subscribe', 'climate_topic') \\\n",
    "    .option(\"failOnDataLoss\", False) \\\n",
    "    .load() \\\n",
    "\n",
    "# Since the incoming data was a json file, we want to parse it into the right format\n",
    "climate_data = climate_data.withColumn(\"value\", climate_data[\"value\"].cast(StringType()))\n",
    "climate_data = climate_data.withColumn(\"data\", from_json(climate_data[\"value\"], climate_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d4061eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns that will be used\n",
    "# Add column geo_hash using geohash function\n",
    "# Modify date to timestamp for watermarking\n",
    "# Specify watermark of 1 day using date\n",
    "climate_stream = climate_data.select(\n",
    "    \"data.date\",\n",
    "    \"data.latitude\",\n",
    "    \"data.longitude\",\n",
    "    \"data.air_temperature\",\n",
    "    \"data.relative_humidity\",\n",
    "    \"data.windspeed_knots\",\n",
    "    \"data.max_wind_speed\",\n",
    "    \"data.precipitation_flag\",\n",
    "    \"data.precipitation\",\n",
    "    \"data.GHI\",\n",
    "    \"data.producer\"\n",
    ").withColumn(\"geo_hash\", geohash_udf(col(\"latitude\"), col(\"longitude\"))) \\\n",
    ".withColumn(\"date\", to_timestamp(\"date\",\"yyyy/MM/dd\")) \\\n",
    ".withWatermark(\"date\", \"1 day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a07ebfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"starting\")\n",
    "# receive_data(climate_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0c27a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the hotspot data from Kafka\n",
    "hotspot_data = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', f'{ip_address}:9092') \\\n",
    "    .option('subscribe', 'hotspot_topic') \\\n",
    "    .option(\"failOnDataLoss\", False) \\\n",
    "    .load() \\\n",
    "\n",
    "# Since the incoming data was a json file, we want to parse it into the right format\n",
    "hotspot_data = hotspot_data.withColumn(\"value\", hotspot_data[\"value\"].cast(StringType()))\n",
    "hotspot_data = hotspot_data.withColumn(\"data\", from_json(hotspot_data[\"value\"], hotspot_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d2362ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns that will be used\n",
    "# Add column geo_hash using geohash function\n",
    "# Modify date to timestamp for watermarking\n",
    "# Also modify datetime to timestamp\n",
    "# Specify watermark of 1 day using date\n",
    "hotspot_stream = hotspot_data.select(\n",
    "    \"data.date\",\n",
    "    \"data.datetime\",\n",
    "    col(\"data.latitude\").alias(\"hotspot_latitude\"),\n",
    "    col(\"data.longitude\").alias(\"hotspot_longitude\"),\n",
    "    \"data.confidence\",\n",
    "    \"data.surface_temperature\",\n",
    "    col(\"data.producer\").alias(\"hotspot_producer\"),\n",
    ").withColumn(\"geo_hash\", geohash_udf(col(\"hotspot_latitude\"), col(\"hotspot_longitude\"))) \\\n",
    ".withColumn(\"date\", to_timestamp(\"date\",\"yyyy/MM/dd\")) \\\n",
    ".withColumn(\"datetime\", to_timestamp(\"datetime\",\"yyyy/MM/dd HH:mm:ss\")) \\\n",
    ".withWatermark(\"date\", \"1 day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "df7dd733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Starting\")\n",
    "# receive_data(hotspot_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9a2658d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the climate data with hotspot data based on date and geo-hashing with precision 3\n",
    "joined_data = climate_stream.join(\n",
    "    hotspot_stream,\n",
    "    [\"date\", \"geo_hash\"],\n",
    "    \"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6c63cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "52d413f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the required columns for the document\n",
    "output_data = joined_data.select(\n",
    "    \"date\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"air_temperature\",\n",
    "    \"relative_humidity\",\n",
    "    \"windspeed_knots\",\n",
    "    \"max_wind_speed\",\n",
    "    \"precipitation_flag\",\n",
    "    \"precipitation\",\n",
    "    \"GHI\",\n",
    "    \"geo_hash\",\n",
    "    struct(\n",
    "        \"datetime\",\n",
    "        \"hotspot_producer\",\n",
    "        \"hotspot_latitude\",\n",
    "        \"hotspot_longitude\",\n",
    "        \"confidence\",\n",
    "        \"surface_temperature\",\n",
    "        when(col(\"surface_temperature\").isNull(), None) \\\n",
    "        .when((col(\"surface_temperature\") > 20) & (col(\"GHI\") > 180), \"natural\") \\\n",
    "        .otherwise(\"other\").alias(\"cause\")\n",
    "    ).alias(\"hotspot_data\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0a1750e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "67f99991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Starting\")\n",
    "# receive_data(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "62c3c718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDBWriter class for writing batch data to MongoDB\n",
    "class WriteToMongoDB:\n",
    "    def __init__(self):\n",
    "        # Set up the MongoDB connection\n",
    "        self.client = MongoClient(ip_address, 27017)\n",
    "        self.db = self.client[\"A3_db\"]\n",
    "        self.collection = self.db[\"ClimateHotspot\"]\n",
    "\n",
    "    def __call__(self, batch_df, batch_id):\n",
    "        # Convert the batch DataFrame to a list of json\n",
    "        json_documents = batch_df.toJSON().collect()\n",
    "    \n",
    "        # Convert each JSON string to a dictionary\n",
    "        for json_doc in json_documents:\n",
    "            document = json.loads(json_doc)\n",
    "            print(document)\n",
    "            print()\n",
    "            document_date = document['date']\n",
    "            document_geohash = document['geo_hash']\n",
    "            document_hotspot = document.pop(\"hotspot_data\")\n",
    "            \n",
    "            # If fire record exist, update or insert based on match query\n",
    "            # Update if climate data (parent) already exist, otherwise insert new document\n",
    "            if document_hotspot:\n",
    "                self.collection.update_one({\"date\":document_date, \"geo_hash\":document_geohash},\n",
    "                                          {\"$push\":{\"hotspot_data\":document_hotspot},\n",
    "                                           \"$setOnInsert\":document\n",
    "                                          },upsert=True)\n",
    "            # If fire record doesn't exist, simply insert if document doesn't exist yet\n",
    "            else:\n",
    "                self.collection.update_one({\"date\":document_date, \"geo_hash\":document_geohash},\n",
    "                          {\"$setOnInsert\":document},upsert=True)\n",
    "    \n",
    "    def close(self):\n",
    "        # Close the MongoDB connection\n",
    "        self.client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cdab4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the streaming data to MongoDB\n",
    "output_query = output_data.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .foreachBatch(WriteToMongoDB()) \\\n",
    "    .outputMode(\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "da159e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 272, in call\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 269, in call\n",
      "    self.func(DataFrame(jdf, self.session), batch_id)\n",
      "  File \"/tmp/ipykernel_70/3862229099.py\", line 11, in __call__\n",
      "    json_documents = batch_df.toJSON().collect()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/rdd.py\", line 1197, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": java.lang.InterruptedException\n",
      "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1048)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:943)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor152.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy36.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:660)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:255)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query = output_query.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted\")\n",
    "finally:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b740377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
